{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de Algoritmos de Machine Learning No Supervisado\n",
    "\n",
    "Este notebook contiene ejemplos breves y didácticos de entrenamiento y evaluación para varios algoritmos no supervisados de `scikit-learn`. Los datos utilizados provienen del archivo `penguins_subset.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías y Cargar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install umap-learn mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import umap\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"../data/penguins/penguins.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar características numéricas y eliminar valores nulos\n",
    "X = df[['Flipper Length (mm)', 'Body Mass (g)']].dropna()\n",
    "\n",
    "# Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means\n",
    "\n",
    "- Agrupa datos en k clústeres basados en la distancia euclidiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster_KMeans'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluación: Inercia y Silhouette Score\n",
    "print(f\"Inercia: {kmeans.inertia_}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, df['Cluster_KMeans'])}\")\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['Cluster_KMeans'], cmap='viridis')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN\n",
    "- Clustering basado en densidad que identifica regiones densas de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "df['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluación: Contar puntos en cada clúster\n",
    "print(df['Cluster_DBSCAN'].value_counts())\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['Cluster_DBSCAN'], cmap='viridis')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "- Agrupa datos en una jerarquía de clústeres mediante un enfoque aglomerativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar clustering jerárquico\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "df['Cluster_Agg'] = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluación: Silhouette Score\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, df['Cluster_Agg'])}\")\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['Cluster_Agg'], cmap='viridis')\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Mixture Model (GMM)\n",
    "- Modelo probabilístico que asume que los datos son generados por una mezcla de distribuciones gaussianas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar GMM\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "df['Cluster_GMM'] = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluación: Log-verosimilitud\n",
    "print(f\"Log-verosimilitud: {gmm.score(X_scaled)}\")\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['Cluster_GMM'], cmap='viridis')\n",
    "plt.title(\"Gaussian Mixture Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apriori (Association)\n",
    "- Algoritmo de asociación que identifica conjuntos de elementos frecuentes en transacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo (compras de productos)\n",
    "data = {'Producto': ['Manzana', 'Banana', 'Leche', 'Pan', 'Manzana', 'Banana', 'Leche'],\n",
    "        'Transacción': [1, 1, 1, 2, 2, 3, 3]}\n",
    "basket = pd.DataFrame(data).pivot_table(index='Transacción', columns='Producto', aggfunc=len, fill_value=0)\n",
    "\n",
    "# Entrenar Apriori\n",
    "frequent_itemsets = apriori(basket, min_support=0.5, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Mostrar reglas de asociación\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Principal Component Analysis (PCA)\n",
    "- Técnica de reducción de dimensionalidad que transforma los datos en componentes principales ortogonales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Evaluación: Varianza explicada\n",
    "print(f\"Varianza explicada: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. t-SNE\n",
    "- Técnica de reducción de dimensionalidad no lineal para visualización de datos complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.title(\"t-SNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. UMAP\n",
    "- Técnica de reducción de dimensionalidad similar a t-SNE pero más rápida y escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_scaled)\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1])\n",
    "plt.title(\"UMAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Non-Negative Matrix Factorization (NMF)\n",
    "- Descompone una matriz en factores no negativos para extraer patrones interpretables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los valores deben estar entre 0-1\n",
    "X_scaled = np.abs(X_scaled)\n",
    "\n",
    "# Entrenar NMF\n",
    "nmf = NMF(n_components=2, random_state=42)\n",
    "W = nmf.fit_transform(X_scaled)\n",
    "\n",
    "# Evaluación: Reconstrucción de los datos\n",
    "H = nmf.components_\n",
    "print(f\"Error de reconstrucción: {((X_scaled - W @ H) ** 2).mean()}\")\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(W[:, 0], W[:, 1])\n",
    "plt.title(\"NMF\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
