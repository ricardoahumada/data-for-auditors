{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dask: De Conceptos a Aplicaciones Prácticas**\n",
        "\n",
        "## Objetivos del Curso\n",
        "- Comprender qué es Dask y por qué es fundamental para big data\n",
        "- Dominar los conceptos de computación distribuida y paralelización\n",
        "- Procesar datasets grandes en máquina local\n",
        "- Aplicar Dask en casos prácticos reales\n",
        "- Resolver problemas comunes y errores frecuentes\n",
        "\n",
        "## Estructura del Curso\n",
        "1. Introducción y Fundamentos (15 min) - Qué es Dask?\n",
        "2. Conceptos Técnicos (20 min) - Lazy Evaluation y Particiones\n",
        "3. Comparación Práctica (15 min) - Pandas vs Dask\n",
        "4. Caso Práctico (20 min) - Dataset de Vuelos Real\n",
        "5. Optimización Avanzada (10 min) - Mejores Prácticas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6968a10a",
      "metadata": {},
      "source": [
        "## 1. INTRODUCCIÓN: QUÉ ES DASK?\n",
        "\n",
        "Dask es una biblioteca paralela y distribuida en Python que permite escalar cálculos comunes a conjuntos de datos más grandes de los que pueden manejar pandas o numpy directamente.\n",
        "\n",
        "### Qué problemas resuelve Dask?\n",
        "\n",
        "- Memoria limitada: Procesar datos más grandes que la RAM disponible\n",
        "- CPU subutilizada: Aprovechar todos los núcleos del procesador\n",
        "- Escalabilidad: Desde laptop personal hasta clusters distribuidos\n",
        "- API familiar: Sintaxis similar a pandas sin curva de aprendizaje pronunciada\n",
        "\n",
        "### Ventajas principales:\n",
        "- Sintaxis similar a pandas y numpy\n",
        "- Manejo eficiente de datos que no entran en memoria\n",
        "- Paralelismo automático y distribución inteligente\n",
        "- Integración con múltiples fuentes de datos\n",
        "- Ecosistema completo (DataFrame, Array, ML, Delayed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalación\n",
        "\n",
        "Si aún no tienes instalado dask, ejecuta:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install dask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También puedes instalarlo junto con otras dependencias útiles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install dask[dataframe]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparación: Dask vs Pandas\n",
        "\n",
        "| Característica | Pandas | Dask |\n",
        "|----------------|--------|------|\n",
        "| Datos en RAM | Sí (todo en memoria) | No (lazy evaluation) |\n",
        "| Paralelización | No (procesamiento secuencial) | Sí (múltiples núcleos) |\n",
        "| Tamaño máximo | Limitado por RAM | Sí (terabytes) |\n",
        "| API familiar | - | Sí (muy similar) |\n",
        "| Velocidad | Rápido para datasets pequeños | Más lento por operación, pero escalable |\n",
        "| Memoria | Carga todo inmediatamente | Carga bajo demanda |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. IMPORTACIÓN Y CONFIGURACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import dask as dk  # Dask principal para obtener versión\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"Versión de pandas: {pd.__version__}\")\n",
        "print(f\"Versión de dask: {dk.__version__}\")\n",
        "print(\"Imports completados exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. CONCEPTOS FUNDAMENTALES\n",
        "\n",
        "### A. Lazy Evaluation (Evaluación Perezosa)\n",
        "\n",
        "Concepto clave: Dask no ejecuta operaciones inmediatamente, las construye en un grafo de tareas que se ejecuta solo cuando:\n",
        "- Llamamos .compute()\n",
        "- Guardamos a disco\n",
        "- Mostramos resultados (trigger implícito)\n",
        "\n",
        "Esta estrategia permite:\n",
        "- Inicialización rápida (no carga datos inmediatamente)\n",
        "- Optimización global del plan de ejecución\n",
        "- Planificación inteligente de operaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demostración de Lazy Evaluation\n",
        "print(\"Demostrando Lazy Evaluation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Crear un DataFrame de Dask (no se carga en memoria aún)\n",
        "n_partitions = 4\n",
        "df_demo = dd.from_pandas(\n",
        "    pd.DataFrame({\n",
        "        'id': range(100000),\n",
        "        'valor': np.random.randn(100000),\n",
        "        'categoria': np.random.choice(['A', 'B', 'C'], 100000),\n",
        "        'fecha': pd.date_range('2023-01-01', periods=100000, freq='1min')\n",
        "    }),\n",
        "    npartitions=n_partitions\n",
        ")\n",
        "\n",
        "creation_time = time.time() - start_time\n",
        "print(f\"DataFrame creado en {creation_time:.4f} segundos\")\n",
        "print(f\"Tipo: {type(df_demo)}\")\n",
        "print(f\"Particiones: {df_demo.npartitions}\")\n",
        "\n",
        "print(\"\\nPlan de ejecución (lazy - SIN ejecutar):\")\n",
        "print(df_demo)  # Solo muestra el plan, no ejecuta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trigger la ejecución con .compute()\n",
        "print(\"Ejecutando .compute()...\")\n",
        "start_time = time.time()\n",
        "\n",
        "result = df_demo.compute()\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "print(f\"Ejecutado en {execution_time:.2f} segundos\")\n",
        "print(f\"Forma del resultado: {result.shape}\")\n",
        "print(f\"Tipo: {type(result)}\")\n",
        "\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CASO PRÁCTICO: DATASET DE VUELOS\n",
        "\n",
        "### Análisis de Datos de Aviación con Dask\n",
        "\n",
        "Usaremos un dataset real de vuelos para demostrar el poder de Dask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# URLs de datasets de vuelos (de diferentes tamaños)\n",
        "print(\"CONFIGURACIÓN DE DATOS DE VUELOS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "big_csv=\"https://github.com/ricardoahumada/Python_for_Data_Science/raw/refs/heads/master/data/2008.zip\"\n",
        "small_csv=\"https://github.com/ricardoahumada/Python_for_Data_Science/raw/refs/heads/master/data/2008_small.zip\"\n",
        "very_small_csv = 'https://github.com/ricardoahumada/data-for-auditors/raw/refs/heads/main/4.%20An%C3%A1lisis%20Masivo%20de%20Datos/Optimizacion/data/2008_very_small.csv'\n",
        "\n",
        "print(\"Dataset disponible: vuelos pequeños (~7MB) para demo\")\n",
        "print(\"\\nPara archivos grandes, Dask necesita dependencias adicionales:\")\n",
        "print(\"# !pip install requests aiohttp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset con Dask (dataset muy pequeño para demo)\n",
        "print(\"Cargando dataset con Dask...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    df_vuelos = dd.read_csv(small_csv, dtype={'CancellationCode': 'object'})\n",
        "    load_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Dataset cargado en {load_time:.2f} segundos\")\n",
        "    print(f\"Tipo: {type(df_vuelos)}\")\n",
        "    print(f\"Particiones: {df_vuelos.npartitions}\")\n",
        "    \n",
        "    # Ver estructura sin ejecutar completamente\n",
        "    print(f\"\\nPlan de ejecución:\")\n",
        "    print(df_vuelos)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error cargando dataset: {e}\")\n",
        "    print(\"Usando dataset sintético alternativo...\")\n",
        "    \n",
        "    # Dataset sintético alternativo\n",
        "    df_vuelos = dd.from_pandas(\n",
        "        pd.DataFrame({\n",
        "            'Year': np.random.randint(2008, 2009, 10000),\n",
        "            'Month': np.random.randint(1, 13, 10000),\n",
        "            'DayofMonth': np.random.randint(1, 32, 10000),\n",
        "            'DayOfWeek': np.random.randint(1, 8, 10000),\n",
        "            'DepDelay': np.random.randint(-30, 300, 10000),\n",
        "            'ArrDelay': np.random.randint(-30, 300, 10000),\n",
        "            'Origin': np.random.choice(['JFK', 'LAX', 'ORD', 'ATL', 'DFW'], 10000),\n",
        "            'Dest': np.random.choice(['JFK', 'LAX', 'ORD', 'ATL', 'DFW'], 10000),\n",
        "            'Distance': np.random.randint(100, 3000, 10000)\n",
        "        }),\n",
        "        npartitions=4\n",
        "    )\n",
        "    print(\"Dataset sintético de vuelos creado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar estructura del DataFrame\n",
        "print(\"ESTRUCTURA DEL DATASET DE VUELOS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Mostrar las primeras filas (ejecuta el plan)\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df_vuelos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Información de columnas\n",
        "print(\"\\nColumnas disponibles:\")\n",
        "print(df_vuelos.columns.tolist())\n",
        "\n",
        "print(\"\\nTipos de datos:\")\n",
        "print(df_vuelos.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de Vuelos con Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis de retrasos de vuelos\n",
        "print(\"ANÁLISIS DE VUELOS CON DASK\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Filtrar vuelos con retraso\n",
        "print(\"\\n1. Vuelos con retraso de salida > 3 minutos:\")\n",
        "start = time.time()\n",
        "vuelos_retrasados = df_vuelos[df_vuelos['DepDelay'] > 3]\n",
        "resultado = vuelos_retrasados.compute()\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(f\"Vuelos con retraso: {len(resultado):,} de {len(df_vuelos.compute()):,} total\")\n",
        "print(f\"Porcentaje: {(len(resultado)/len(df_vuelos.compute()))*100:.1f}%\")\n",
        "\n",
        "# 2. Seleccionar columnas específicas\n",
        "print(\"\\n2. Seleccionar columnas de fecha:\")\n",
        "fecha_cols = df_vuelos[['Year', 'Month', 'DayofMonth', 'DayOfWeek']]\n",
        "print(\"Columnas seleccionadas: Year, Month, DayofMonth, DayOfWeek\")\n",
        "fecha_cols.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Calcular promedios de retraso\n",
        "print(\"\\n3. Análisis de retrasos:\")\n",
        "start = time.time()\n",
        "\n",
        "promedio_dep_delay = df_vuelos['DepDelay'].mean().compute()\n",
        "promedio_arr_delay = df_vuelos['ArrDelay'].mean().compute()\n",
        "\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(f\"Retraso promedio salida: {promedio_dep_delay:.2f} minutos\")\n",
        "print(f\"Retraso promedio llegada: {promedio_arr_delay:.2f} minutos\")\n",
        "\n",
        "# 4. Agrupamiento por aeropuerto de origen\n",
        "print(\"\\n4. Retrasos por aeropuerto de origen:\")\n",
        "start = time.time()\n",
        "retrasos_origen = df_vuelos.groupby('Origin')['DepDelay'].mean().compute()\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(\"\\nRetrasos promedio por aeropuerto:\")\n",
        "for aeropuerto, retraso in retrasos_origen.sort_values(ascending=False).items():\n",
        "    print(f\"   {aeropuerto}: {retraso:.2f} minutos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. COMPARACIÓN PRÁCTICA: PANDAS VS DASK\n",
        "\n",
        "### Performance en Dataset Real\n",
        "\n",
        "Vamos a comparar el rendimiento en el mismo dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparación directa Pandas vs Dask\n",
        "print(\"COMPARACIÓN DE PERFORMANCE: PANDAS VS DASK\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Convertir a pandas para comparación\n",
        "print(\"Convirtiendo a pandas para comparación...\")\n",
        "df_pandas = df_vuelos.compute()\n",
        "print(f\"Dataset pandas: {df_pandas.shape}\")\n",
        "\n",
        "print(\"\\nComparación de operaciones básicas:\")\n",
        "\n",
        "# Operación 1: Filtrado\n",
        "print(\"\\n1. FILTRADO: Vuelos con DepDelay > 15\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_filtro = df_pandas[df_pandas['DepDelay'] > 15]\n",
        "pandas_time = time.time() - start\n",
        "pandas_result_count = len(pandas_filtro)\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_filtro = df_vuelos[df_vuelos['DepDelay'] > 15].compute()\n",
        "dask_time = time.time() - start\n",
        "dask_result_count = len(dask_filtro)\n",
        "\n",
        "print(f\"   Pandas: {pandas_time:.4f} segundos - {pandas_result_count:,} resultados\")\n",
        "print(f\"   Dask:   {dask_time:.4f} segundos - {dask_result_count:,} resultados\")\n",
        "print(f\"   Ganador: {'Dask' if dask_time < pandas_time else 'Pandas'}\")\n",
        "\n",
        "# Operación 2: GroupBy\n",
        "print(\"\\n2. GROUPBY: Retrasos por aeropuerto\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_groupby = df_pandas.groupby('Origin')['DepDelay'].mean()\n",
        "pandas_time = time.time() - start\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_groupby = df_vuelos.groupby('Origin')['DepDelay'].mean().compute()\n",
        "dask_time = time.time() - start\n",
        "\n",
        "print(f\"   Pandas: {pandas_time:.4f} segundos\")\n",
        "print(f\"   Dask:   {dask_time:.4f} segundos\")\n",
        "print(f\"   Ganador: {'Dask' if dask_time < pandas_time else 'Pandas'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. OPTIMIZACIÓN AVANZADA\n",
        "\n",
        "### Técnicas para Maximizar el Rendimiento de Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimización 1: Tipos de datos eficientes\n",
        "print(\"OPTIMIZACIÓN: TIPOS DE DATOS\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "print(\"\\nTipos de datos originales:\")\n",
        "print(df_vuelos.dtypes)\n",
        "\n",
        "# Optimizar tipos de datos\n",
        "print(\"\\nOptimizando tipos de datos...\")\n",
        "start = time.time()\n",
        "\n",
        "# Convertir columnas categóricas\n",
        "df_optimized = df_vuelos.copy()\n",
        "if 'Origin' in df_optimized.columns:\n",
        "    df_optimized['Origin'] = df_optimized['Origin'].astype('category')\n",
        "if 'Dest' in df_optimized.columns:\n",
        "    df_optimized['Dest'] = df_optimized['Dest'].astype('category')\n",
        "\n",
        "# Convertir enteros grandes a tipos más pequeños\n",
        "int_columns = ['Year', 'Month', 'DayofMonth', 'DayOfWeek']\n",
        "for col in int_columns:\n",
        "    if col in df_optimized.columns:\n",
        "        df_optimized[col] = df_optimized[col].astype('int8')\n",
        "\n",
        "# Convertir delays a float32 para ahorrar memoria\n",
        "float_columns = ['DepDelay', 'ArrDelay']\n",
        "for col in float_columns:\n",
        "    if col in df_optimized.columns:\n",
        "        df_optimized[col] = df_optimized[col].astype('float32')\n",
        "\n",
        "optimization_time = time.time() - start\n",
        "print(f\"Optimización completada en {optimization_time:.3f} segundos\")\n",
        "\n",
        "print(\"\\nTipos de datos optimizados:\")\n",
        "print(df_optimized.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c8482a",
      "metadata": {},
      "source": [
        "## 7. DEMOSTRACIÓN PRÁCTICA: DATASET DE 1GB\n",
        "\n",
        "### Objetivo: Procesar un dataset realista de ~1GB\n",
        "\n",
        "Vamos a:\n",
        "1. **Generar** un dataset de 1GB de datos de e-commerce\n",
        "2. **Analizar** patrones de comportamiento de usuarios\n",
        "3. **Optimizar** queries usando las mejores prácticas de Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca82253",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para generar dataset grande - VERSIÓN CORREGIDA\n",
        "def generar_dataset_ecommerce(tamaño_gb=0.5):\n",
        "    \"\"\"\n",
        "    Genera un dataset de e-commerce del tamaño especificado - VERSIÓN SIN ERRORES\n",
        "    Estimación: ~100MB por millón de filas\n",
        "    \"\"\"\n",
        "    print(f\"🚀 Generando dataset de {tamaño_gb}GB...\")\n",
        "    \n",
        "    # Calcular número de filas basado en tamaño deseado\n",
        "    filas_por_mb = 10000  # Estimación\n",
        "    total_filas = int(tamaño_gb * 1000 * filas_por_mb)  # 100MB por millón de filas\n",
        "    \n",
        "    print(f\"Generando {total_filas:,} filas...\")\n",
        "    \n",
        "    # Crear datos sintéticos realistas\n",
        "    np.random.seed(42)  # Para reproducibilidad\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Generar en chunks para memoria eficiente\n",
        "    chunk_size = 100000\n",
        "    chunks = []\n",
        "    \n",
        "    productos = ['Laptop', 'Smartphone', 'Tablet', 'Auriculares', 'Monitor', \n",
        "                'Teclado', 'Mouse', 'Impresora', 'Webcam', 'Micrófono',\n",
        "                'Smartwatch', 'Cámara', 'Parlantes', 'Disco Duro', 'RAM']\n",
        "    \n",
        "    categorias = ['Electrónicos', 'Accesorios', 'Computadoras', 'Audio', 'Video']\n",
        "    \n",
        "    usuarios = [f'usuario_{i}' for i in range(1, 100001)]  # 100k usuarios únicos\n",
        "    \n",
        "    for i in range(0, total_filas, chunk_size):\n",
        "        current_size = min(chunk_size, total_filas - i)\n",
        "        \n",
        "        # ✅ CORRECCIÓN: Generar fechas directamente sin shift()\n",
        "        # En lugar de usar shift() que causaba problemas con arrays\n",
        "        fecha_base = pd.Timestamp('2023-01-01')\n",
        "        # Generar offsets aleatorios en minutos\n",
        "        offsets_minutos = np.random.randint(-365*24*60, 0, current_size)\n",
        "        fechas_compra = pd.to_datetime(fecha_base + pd.to_timedelta(offsets_minutos, unit='m'))\n",
        "        \n",
        "        chunk = pd.DataFrame({\n",
        "            'user_id': np.random.choice(usuarios, current_size),\n",
        "            'producto': np.random.choice(productos, current_size),\n",
        "            'categoria': np.random.choice(categorias, current_size),\n",
        "            'precio': np.random.uniform(20, 2000, current_size).round(2),\n",
        "            'cantidad': np.random.poisson(2, current_size) + 1,  # Poisson para naturalidad\n",
        "            'descuento': np.random.uniform(0, 0.3, current_size).round(2),  # 0-30% descuento\n",
        "            'fecha_compra': fechas_compra,  # ✅ SOLUCIÓN: Fechas generadas directamente\n",
        "            'pais': np.random.choice(['España', 'Francia', 'Alemania', 'Italia', 'Portugal'], current_size),\n",
        "            'metodo_pago': np.random.choice(['Tarjeta', 'PayPal', 'Transferencia', 'Bizum'], current_size),\n",
        "            'rating': np.random.randint(1, 6, current_size)  # 1-5 estrellas\n",
        "        })\n",
        "        \n",
        "        # Calcular precio final con descuento\n",
        "        chunk['precio_final'] = (chunk['precio'] * (1 - chunk['descuento']) * chunk['cantidad']).round(2)\n",
        "        \n",
        "        chunks.append(chunk)\n",
        "        \n",
        "        if i % 1000000 == 0 and i > 0:\n",
        "            print(f\"  Generadas {i:,} filas...\")\n",
        "    \n",
        "    # Combinar chunks\n",
        "    df_completo = pd.concat(chunks, ignore_index=True)\n",
        "    \n",
        "    # Calcular tamaño real\n",
        "    tamaño_mb = df_completo.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "    \n",
        "    print(f\"✅ Dataset generado en {time.time() - start_time:.2f} segundos\")\n",
        "    print(f\"📊 Filas: {len(df_completo):,}\")\n",
        "    print(f\"💾 Tamaño en memoria: {tamaño_mb:.1f} MB\")\n",
        "    \n",
        "    return df_completo\n",
        "\n",
        "print(\"✅ Función corregida lista - Ya no hay errores de array ambiguity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1169dcbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar dataset (AHORA SIN ERRORES)\n",
        "print(\"🔥 GENERANDO DATASET...\")\n",
        "df_ecommerce = generar_dataset_ecommerce(tamaño_gb=0.5)  # 500MB por tiempo del curso\n",
        "print(f\"\\nDataset listo para análisis: {df_ecommerce.shape}\")\n",
        "print(f\"📋 Columnas: {list(df_ecommerce.columns)}\")\n",
        "print(f\"\\n🔍 Primeras 3 filas:\")\n",
        "df_ecommerce.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d3640c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir a DataFrame de Dask\n",
        "print(\"🔄 Convirtiendo a DataFrame de Dask...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Dividir en particiones basadas en el número de CPUs\n",
        "import os\n",
        "n_cores = os.cpu_count() or 4\n",
        "n_partitions = min(n_cores * 2, 8)  # Doble número de cores, máximo 8\n",
        "\n",
        "print(f\"Usando {n_partitions} particiones en {n_cores} cores\")\n",
        "\n",
        "df_dask_ecommerce = dd.from_pandas(df_ecommerce, npartitions=n_partitions)\n",
        "\n",
        "print(f\"✅ Conversión completada en {time.time() - start_time:.2f} segundos\")\n",
        "print(f\"Forma: {df_dask_ecommerce.shape}\")\n",
        "print(f\"Particiones: {df_dask_ecommerce.npartitions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a342cf0",
      "metadata": {},
      "source": [
        "### Análisis 1: Performance Comparison\n",
        "\n",
        "**Compararemos la velocidad entre Pandas y Dask:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8219f6c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparación de performance: Pandas vs Dask\n",
        "print(\"⚡ COMPARACIÓN DE PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Operación 1: Filtrado simple\n",
        "print(\"\\n1. Filtrado de compras > $100:\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_result = df_ecommerce[df_ecommerce['precio_final'] > 100]\n",
        "pandas_time = time.time() - start\n",
        "print(f\"   Pandas: {pandas_time:.2f} segundos - {len(pandas_result):,} resultados\")\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_result = df_dask_ecommerce[df_dask_ecommerce['precio_final'] > 100].compute()\n",
        "dask_time = time.time() - start\n",
        "print(f\"   Dask:   {dask_time:.2f} segundos - {len(dask_result):,} resultados\")\n",
        "\n",
        "print(f\"   🚀 Dask es {pandas_time/dask_time:.1f}x más rápido\" if dask_time < pandas_time else f\"   🐌 Pandas es {dask_time/pandas_time:.1f}x más rápido\")\n",
        "\n",
        "# Operación 2: GroupBy complejo\n",
        "print(\"\\n2. Ventas por categoría y país:\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_groupby = df_ecommerce.groupby(['categoria', 'pais'])['precio_final'].agg(['sum', 'mean', 'count'])\n",
        "pandas_time = time.time() - start\n",
        "print(f\"   Pandas: {pandas_time:.2f} segundos\")\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_groupby = df_dask_ecommerce.groupby(['categoria', 'pais'])['precio_final'].agg(['sum', 'mean', 'count']).compute()\n",
        "dask_time = time.time() - start\n",
        "print(f\"   Dask:   {dask_time:.2f} segundos\")\n",
        "\n",
        "print(f\"   🚀 Dask es {pandas_time/dask_time:.1f}x más rápido\" if dask_time < pandas_time else f\"   🐌 Pandas es {dask_time/pandas_time:.1f}x más rápido\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1503646b",
      "metadata": {},
      "source": [
        "### Análisis 2: Insights de Negocio\n",
        "\n",
        "**Vamos a extraer insights útiles del dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8d3492",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis de negocio con Dask\n",
        "print(\"📈 ANÁLISIS DE NEGOCIO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Top productos por ingresos\n",
        "print(\"\\n🏆 Top 5 productos por ingresos totales:\")\n",
        "start = time.time()\n",
        "top_productos = df_dask_ecommerce.groupby('producto')['precio_final'].sum().nlargest(5).compute()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "for i, (producto, ingresos) in enumerate(top_productos.items(), 1):\n",
        "    print(f\"   {i}. {producto}: ${ingresos:,.2f}\")\n",
        "\n",
        "# 2. Análisis temporal\n",
        "print(\"\\n📅 Ingresos por mes:\")\n",
        "start = time.time()\n",
        "df_dask_ecommerce['mes'] = df_dask_ecommerce['fecha_compra'].dt.to_period('M')\n",
        "ingresos_mes = df_dask_ecommerce.groupby('mes')['precio_final'].sum().compute()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "print(\"Primeros 5 meses:\")\n",
        "print(ingresos_mes.head())\n",
        "\n",
        "# 3. Segmentación de clientes\n",
        "print(\"\\n👥 Segmentación por gasto total:\")\n",
        "start = time.time()\n",
        "gasto_usuarios = df_dask_ecommerce.groupby('user_id')['precio_final'].sum().compute()\n",
        "gasto_usuarios = gasto_usuarios.reset_index()\n",
        "gasto_usuarios['segmento'] = pd.cut(gasto_usuarios['precio_final'], \n",
        "                                   bins=[0, 100, 500, 1000, float('inf')], \n",
        "                                   labels=['Bajo', 'Medio', 'Alto', 'VIP'])\n",
        "segmentos = gasto_usuarios['segmento'].value_counts()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "print(\"Distribución de segmentos:\")\n",
        "for segmento, cantidad in segmentos.items():\n",
        "    porcentaje = (cantidad / len(gasto_usuarios)) * 100\n",
        "    print(f\"   {segmento}: {cantidad:,} usuarios ({porcentaje:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f497f0",
      "metadata": {},
      "source": [
        "## 8. EJERCICIO FINAL: TU TURNO\n",
        "\n",
        "### **Tiempo: 10 minutos**\n",
        "\n",
        "**Objetivo**: Analizar patrones de comportamiento de usuarios usando Dask\n",
        "\n",
        "**Dataset**: Usa el mismo `df_dask_ecommerce` que hemos estado trabajando\n",
        "\n",
        "**Tareas**:\n",
        "1. **Identifica** el top 3 de usuarios más rentables\n",
        "2. **Calcula** el valor promedio de transacción por método de pago\n",
        "3. **Encuentra** la correlación entre rating y precio final\n",
        "4. **Determina** qué día de la semana genera más ventas\n",
        "\n",
        "**Instrucciones**:\n",
        "- Escribe el código completo en la celda siguiente\n",
        "- Usa `.compute()` solo cuando sea necesario\n",
        "- Muestra los resultados de forma clara\n",
        "- Explica qué insights obtienes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9692954e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 EJERCICIO FINAL: Análisis de Comportamiento de Usuarios\n",
            "============================================================\n",
            "\n",
            "1. 🏆 TOP 3 USUARIOS MÁS RENTABLES:\n",
            "\n",
            "2. 💳 VALOR PROMEDIO POR MÉTODO DE PAGO:\n",
            "\n",
            "3. ⭐ CORRELACIÓN RATING-PRECIO:\n",
            "\n",
            "4. 📅 MEJOR DÍA DE LA SEMANA:\n",
            "\n",
            "5. 💡 TUS INSIGHTS:\n",
            "\n",
            "✅ ¡Ejercicio completado!\n"
          ]
        }
      ],
      "source": [
        "# 📝 EJERCICIO FINAL - Completa el código\n",
        "\n",
        "print(\"🎯 EJERCICIO FINAL: Análisis de Comportamiento de Usuarios\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tu código aquí:\n",
        "\n",
        "# 1. TOP 3 USUARIOS MÁS RENTABLES\n",
        "print(\"\\n1. 🏆 TOP 3 USUARIOS MÁS RENTABLES:\")\n",
        "# Hint: usa groupby('user_id')['precio_final'].sum() y .nlargest(3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. VALOR PROMEDIO POR MÉTODO DE PAGO\n",
        "print(\"\\n2. 💳 VALOR PROMEDIO POR MÉTODO DE PAGO:\")\n",
        "# Hint: usa groupby('metodo_pago')['precio_final'].mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. CORRELACIÓN ENTRE RATING Y PRECIO\n",
        "print(\"\\n3. ⭐ CORRELACIÓN RATING-PRECIO:\")\n",
        "# Hint: usa .corr() entre las columnas 'rating' y 'precio_final'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. DÍA DE LA SEMANA CON MÁS VENTAS\n",
        "print(\"\\n4. 📅 MEJOR DÍA DE LA SEMANA:\")\n",
        "# Hint: extrae day_name() de fecha_compra y agrupa por ese campo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 💡 INSIGHTS ADICIONALES\n",
        "print(\"\\n5. 💡 TUS INSIGHTS:\")\n",
        "# ¿Qué patrones interesante encuentras?\n",
        "# ¿Qué recomendaciones harías al negocio?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n✅ ¡Ejercicio completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. RESUMEN Y CASOS DE USO\n",
        "\n",
        "### Lo que has aprendido en este curso:\n",
        "\n",
        "1. **Lazy Evaluation**: Dask construye planes de ejecución, no ejecuta inmediatamente\n",
        "2. **Particiones**: Cómo Dask divide los datos para paralelización eficiente\n",
        "3. **API Familiar**: Sintaxis similar a Pandas pero distribuida y escalable\n",
        "4. **Performance**: Cuándo usar Dask vs Pandas según el tamaño del dataset\n",
        "5. **Optimizaciones**: Tipos de datos, particionamiento y limpieza de datos\n",
        "6. **Casos Prácticos**: Análisis real de vuelos con datasets grandes\n",
        "\n",
        "### Casos de uso ideales para Dask:\n",
        "\n",
        "- Datasets de 1GB+ en laptop personal\n",
        "- Análisis de logs y datos de sensores\n",
        "- Procesamiento ETL distribuido\n",
        "- Machine Learning con datasets grandes\n",
        "- Análisis financiero en tiempo real\n",
        "- Procesamiento de datos científicos\n",
        "\n",
        "### Cuándo usar Dask vs Pandas:\n",
        "\n",
        "| Situación | Recomendación |\n",
        "|-----------|---------------|\n",
        "| Dataset < 100MB | Pandas (más rápido) |\n",
        "| Dataset 100MB - 1GB | Dask (si hay múltiples núcleos) |\n",
        "| Dataset > 1GB | Dask (necesario) |\n",
        "| Datasets que no caben en RAM | Dask (lazy loading) |\n",
        "| Análisis simple de una vez | Pandas |\n",
        "| Pipeline complejo distribuido | Dask |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RECURSOS ADICIONALES\n",
        "\n",
        "### Documentación y Aprendizaje\n",
        "\n",
        "- Documentación oficial: https://docs.dask.org/en/stable/\n",
        "- Tutorial interactivo: https://tutorial.dask.org\n",
        "- Comunidad: https://stackoverflow.com/questions/tagged/dask\n",
        "- Ejemplos reales: https://github.com/dask/dask-examples\n",
        "- Guía de migración de Pandas: https://docs.dask.org/en/stable/dataframe-api.html\n",
        "\n",
        "### Próximos Pasos\n",
        "\n",
        "1. **Practica** con tus propios datasets\n",
        "2. **Experimenta** con diferentes números de particiones\n",
        "3. **Aprende** Dask-ML para machine learning\n",
        "4. **Explora** Dask-Delayed para funciones personalizadas\n",
        "5. **Considera** clusters distribuidos para datasets masivos"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
