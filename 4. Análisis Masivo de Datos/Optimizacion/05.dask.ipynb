{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dask: De Conceptos a Aplicaciones Pr√°cticas**\n",
        "\n",
        "## Objetivos del Curso\n",
        "- Comprender qu√© es Dask y por qu√© es fundamental para big data\n",
        "- Dominar los conceptos de computaci√≥n distribuida y paralelizaci√≥n\n",
        "- Procesar datasets grandes en m√°quina local\n",
        "- Aplicar Dask en casos pr√°cticos reales\n",
        "- Resolver problemas comunes y errores frecuentes\n",
        "\n",
        "## Estructura del Curso\n",
        "1. Introducci√≥n y Fundamentos (15 min) - Qu√© es Dask?\n",
        "2. Conceptos T√©cnicos (20 min) - Lazy Evaluation y Particiones\n",
        "3. Comparaci√≥n Pr√°ctica (15 min) - Pandas vs Dask\n",
        "4. Caso Pr√°ctico (20 min) - Dataset de Vuelos Real\n",
        "5. Optimizaci√≥n Avanzada (10 min) - Mejores Pr√°cticas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6968a10a",
      "metadata": {},
      "source": [
        "## 1. INTRODUCCI√ìN: QU√â ES DASK?\n",
        "\n",
        "Dask es una biblioteca paralela y distribuida en Python que permite escalar c√°lculos comunes a conjuntos de datos m√°s grandes de los que pueden manejar pandas o numpy directamente.\n",
        "\n",
        "### Qu√© problemas resuelve Dask?\n",
        "\n",
        "- Memoria limitada: Procesar datos m√°s grandes que la RAM disponible\n",
        "- CPU subutilizada: Aprovechar todos los n√∫cleos del procesador\n",
        "- Escalabilidad: Desde laptop personal hasta clusters distribuidos\n",
        "- API familiar: Sintaxis similar a pandas sin curva de aprendizaje pronunciada\n",
        "\n",
        "### Ventajas principales:\n",
        "- Sintaxis similar a pandas y numpy\n",
        "- Manejo eficiente de datos que no entran en memoria\n",
        "- Paralelismo autom√°tico y distribuci√≥n inteligente\n",
        "- Integraci√≥n con m√∫ltiples fuentes de datos\n",
        "- Ecosistema completo (DataFrame, Array, ML, Delayed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalaci√≥n\n",
        "\n",
        "Si a√∫n no tienes instalado dask, ejecuta:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install dask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi√©n puedes instalarlo junto con otras dependencias √∫tiles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install dask[dataframe]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparaci√≥n: Dask vs Pandas\n",
        "\n",
        "| Caracter√≠stica | Pandas | Dask |\n",
        "|----------------|--------|------|\n",
        "| Datos en RAM | S√≠ (todo en memoria) | No (lazy evaluation) |\n",
        "| Paralelizaci√≥n | No (procesamiento secuencial) | S√≠ (m√∫ltiples n√∫cleos) |\n",
        "| Tama√±o m√°ximo | Limitado por RAM | S√≠ (terabytes) |\n",
        "| API familiar | - | S√≠ (muy similar) |\n",
        "| Velocidad | R√°pido para datasets peque√±os | M√°s lento por operaci√≥n, pero escalable |\n",
        "| Memoria | Carga todo inmediatamente | Carga bajo demanda |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. IMPORTACI√ìN Y CONFIGURACI√ìN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import dask as dk  # Dask principal para obtener versi√≥n\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"Versi√≥n de pandas: {pd.__version__}\")\n",
        "print(f\"Versi√≥n de dask: {dk.__version__}\")\n",
        "print(\"Imports completados exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. CONCEPTOS FUNDAMENTALES\n",
        "\n",
        "### A. Lazy Evaluation (Evaluaci√≥n Perezosa)\n",
        "\n",
        "Concepto clave: Dask no ejecuta operaciones inmediatamente, las construye en un grafo de tareas que se ejecuta solo cuando:\n",
        "- Llamamos .compute()\n",
        "- Guardamos a disco\n",
        "- Mostramos resultados (trigger impl√≠cito)\n",
        "\n",
        "Esta estrategia permite:\n",
        "- Inicializaci√≥n r√°pida (no carga datos inmediatamente)\n",
        "- Optimizaci√≥n global del plan de ejecuci√≥n\n",
        "- Planificaci√≥n inteligente de operaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demostraci√≥n de Lazy Evaluation\n",
        "print(\"Demostrando Lazy Evaluation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Crear un DataFrame de Dask (no se carga en memoria a√∫n)\n",
        "n_partitions = 4\n",
        "df_demo = dd.from_pandas(\n",
        "    pd.DataFrame({\n",
        "        'id': range(100000),\n",
        "        'valor': np.random.randn(100000),\n",
        "        'categoria': np.random.choice(['A', 'B', 'C'], 100000),\n",
        "        'fecha': pd.date_range('2023-01-01', periods=100000, freq='1min')\n",
        "    }),\n",
        "    npartitions=n_partitions\n",
        ")\n",
        "\n",
        "creation_time = time.time() - start_time\n",
        "print(f\"DataFrame creado en {creation_time:.4f} segundos\")\n",
        "print(f\"Tipo: {type(df_demo)}\")\n",
        "print(f\"Particiones: {df_demo.npartitions}\")\n",
        "\n",
        "print(\"\\nPlan de ejecuci√≥n (lazy - SIN ejecutar):\")\n",
        "print(df_demo)  # Solo muestra el plan, no ejecuta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trigger la ejecuci√≥n con .compute()\n",
        "print(\"Ejecutando .compute()...\")\n",
        "start_time = time.time()\n",
        "\n",
        "result = df_demo.compute()\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "print(f\"Ejecutado en {execution_time:.2f} segundos\")\n",
        "print(f\"Forma del resultado: {result.shape}\")\n",
        "print(f\"Tipo: {type(result)}\")\n",
        "\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CASO PR√ÅCTICO: DATASET DE VUELOS\n",
        "\n",
        "### An√°lisis de Datos de Aviaci√≥n con Dask\n",
        "\n",
        "Usaremos un dataset real de vuelos para demostrar el poder de Dask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# URLs de datasets de vuelos (de diferentes tama√±os)\n",
        "print(\"CONFIGURACI√ìN DE DATOS DE VUELOS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "big_csv=\"https://github.com/ricardoahumada/Python_for_Data_Science/raw/refs/heads/master/data/2008.zip\"\n",
        "small_csv=\"https://github.com/ricardoahumada/Python_for_Data_Science/raw/refs/heads/master/data/2008_small.zip\"\n",
        "very_small_csv = 'https://github.com/ricardoahumada/data-for-auditors/raw/refs/heads/main/4.%20An%C3%A1lisis%20Masivo%20de%20Datos/Optimizacion/data/2008_very_small.csv'\n",
        "\n",
        "print(\"Dataset disponible: vuelos peque√±os (~7MB) para demo\")\n",
        "print(\"\\nPara archivos grandes, Dask necesita dependencias adicionales:\")\n",
        "print(\"# !pip install requests aiohttp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset con Dask (dataset muy peque√±o para demo)\n",
        "print(\"Cargando dataset con Dask...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    df_vuelos = dd.read_csv(small_csv, dtype={'CancellationCode': 'object'})\n",
        "    load_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Dataset cargado en {load_time:.2f} segundos\")\n",
        "    print(f\"Tipo: {type(df_vuelos)}\")\n",
        "    print(f\"Particiones: {df_vuelos.npartitions}\")\n",
        "    \n",
        "    # Ver estructura sin ejecutar completamente\n",
        "    print(f\"\\nPlan de ejecuci√≥n:\")\n",
        "    print(df_vuelos)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error cargando dataset: {e}\")\n",
        "    print(\"Usando dataset sint√©tico alternativo...\")\n",
        "    \n",
        "    # Dataset sint√©tico alternativo\n",
        "    df_vuelos = dd.from_pandas(\n",
        "        pd.DataFrame({\n",
        "            'Year': np.random.randint(2008, 2009, 10000),\n",
        "            'Month': np.random.randint(1, 13, 10000),\n",
        "            'DayofMonth': np.random.randint(1, 32, 10000),\n",
        "            'DayOfWeek': np.random.randint(1, 8, 10000),\n",
        "            'DepDelay': np.random.randint(-30, 300, 10000),\n",
        "            'ArrDelay': np.random.randint(-30, 300, 10000),\n",
        "            'Origin': np.random.choice(['JFK', 'LAX', 'ORD', 'ATL', 'DFW'], 10000),\n",
        "            'Dest': np.random.choice(['JFK', 'LAX', 'ORD', 'ATL', 'DFW'], 10000),\n",
        "            'Distance': np.random.randint(100, 3000, 10000)\n",
        "        }),\n",
        "        npartitions=4\n",
        "    )\n",
        "    print(\"Dataset sint√©tico de vuelos creado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar estructura del DataFrame\n",
        "print(\"ESTRUCTURA DEL DATASET DE VUELOS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Mostrar las primeras filas (ejecuta el plan)\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df_vuelos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informaci√≥n de columnas\n",
        "print(\"\\nColumnas disponibles:\")\n",
        "print(df_vuelos.columns.tolist())\n",
        "\n",
        "print(\"\\nTipos de datos:\")\n",
        "print(df_vuelos.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lisis de Vuelos con Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de retrasos de vuelos\n",
        "print(\"AN√ÅLISIS DE VUELOS CON DASK\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Filtrar vuelos con retraso\n",
        "print(\"\\n1. Vuelos con retraso de salida > 3 minutos:\")\n",
        "start = time.time()\n",
        "vuelos_retrasados = df_vuelos[df_vuelos['DepDelay'] > 3]\n",
        "resultado = vuelos_retrasados.compute()\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(f\"Vuelos con retraso: {len(resultado):,} de {len(df_vuelos.compute()):,} total\")\n",
        "print(f\"Porcentaje: {(len(resultado)/len(df_vuelos.compute()))*100:.1f}%\")\n",
        "\n",
        "# 2. Seleccionar columnas espec√≠ficas\n",
        "print(\"\\n2. Seleccionar columnas de fecha:\")\n",
        "fecha_cols = df_vuelos[['Year', 'Month', 'DayofMonth', 'DayOfWeek']]\n",
        "print(\"Columnas seleccionadas: Year, Month, DayofMonth, DayOfWeek\")\n",
        "fecha_cols.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Calcular promedios de retraso\n",
        "print(\"\\n3. An√°lisis de retrasos:\")\n",
        "start = time.time()\n",
        "\n",
        "promedio_dep_delay = df_vuelos['DepDelay'].mean().compute()\n",
        "promedio_arr_delay = df_vuelos['ArrDelay'].mean().compute()\n",
        "\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(f\"Retraso promedio salida: {promedio_dep_delay:.2f} minutos\")\n",
        "print(f\"Retraso promedio llegada: {promedio_arr_delay:.2f} minutos\")\n",
        "\n",
        "# 4. Agrupamiento por aeropuerto de origen\n",
        "print(\"\\n4. Retrasos por aeropuerto de origen:\")\n",
        "start = time.time()\n",
        "retrasos_origen = df_vuelos.groupby('Origin')['DepDelay'].mean().compute()\n",
        "print(f\"Tiempo: {time.time() - start:.3f} segundos\")\n",
        "print(\"\\nRetrasos promedio por aeropuerto:\")\n",
        "for aeropuerto, retraso in retrasos_origen.sort_values(ascending=False).items():\n",
        "    print(f\"   {aeropuerto}: {retraso:.2f} minutos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. COMPARACI√ìN PR√ÅCTICA: PANDAS VS DASK\n",
        "\n",
        "### Performance en Dataset Real\n",
        "\n",
        "Vamos a comparar el rendimiento en el mismo dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparaci√≥n directa Pandas vs Dask\n",
        "print(\"COMPARACI√ìN DE PERFORMANCE: PANDAS VS DASK\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Convertir a pandas para comparaci√≥n\n",
        "print(\"Convirtiendo a pandas para comparaci√≥n...\")\n",
        "df_pandas = df_vuelos.compute()\n",
        "print(f\"Dataset pandas: {df_pandas.shape}\")\n",
        "\n",
        "print(\"\\nComparaci√≥n de operaciones b√°sicas:\")\n",
        "\n",
        "# Operaci√≥n 1: Filtrado\n",
        "print(\"\\n1. FILTRADO: Vuelos con DepDelay > 15\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_filtro = df_pandas[df_pandas['DepDelay'] > 15]\n",
        "pandas_time = time.time() - start\n",
        "pandas_result_count = len(pandas_filtro)\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_filtro = df_vuelos[df_vuelos['DepDelay'] > 15].compute()\n",
        "dask_time = time.time() - start\n",
        "dask_result_count = len(dask_filtro)\n",
        "\n",
        "print(f\"   Pandas: {pandas_time:.4f} segundos - {pandas_result_count:,} resultados\")\n",
        "print(f\"   Dask:   {dask_time:.4f} segundos - {dask_result_count:,} resultados\")\n",
        "print(f\"   Ganador: {'Dask' if dask_time < pandas_time else 'Pandas'}\")\n",
        "\n",
        "# Operaci√≥n 2: GroupBy\n",
        "print(\"\\n2. GROUPBY: Retrasos por aeropuerto\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_groupby = df_pandas.groupby('Origin')['DepDelay'].mean()\n",
        "pandas_time = time.time() - start\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_groupby = df_vuelos.groupby('Origin')['DepDelay'].mean().compute()\n",
        "dask_time = time.time() - start\n",
        "\n",
        "print(f\"   Pandas: {pandas_time:.4f} segundos\")\n",
        "print(f\"   Dask:   {dask_time:.4f} segundos\")\n",
        "print(f\"   Ganador: {'Dask' if dask_time < pandas_time else 'Pandas'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. OPTIMIZACI√ìN AVANZADA\n",
        "\n",
        "### T√©cnicas para Maximizar el Rendimiento de Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizaci√≥n 1: Tipos de datos eficientes\n",
        "print(\"OPTIMIZACI√ìN: TIPOS DE DATOS\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "print(\"\\nTipos de datos originales:\")\n",
        "print(df_vuelos.dtypes)\n",
        "\n",
        "# Optimizar tipos de datos\n",
        "print(\"\\nOptimizando tipos de datos...\")\n",
        "start = time.time()\n",
        "\n",
        "# Convertir columnas categ√≥ricas\n",
        "df_optimized = df_vuelos.copy()\n",
        "if 'Origin' in df_optimized.columns:\n",
        "    df_optimized['Origin'] = df_optimized['Origin'].astype('category')\n",
        "if 'Dest' in df_optimized.columns:\n",
        "    df_optimized['Dest'] = df_optimized['Dest'].astype('category')\n",
        "\n",
        "# Convertir enteros grandes a tipos m√°s peque√±os\n",
        "int_columns = ['Year', 'Month', 'DayofMonth', 'DayOfWeek']\n",
        "for col in int_columns:\n",
        "    if col in df_optimized.columns:\n",
        "        df_optimized[col] = df_optimized[col].astype('int8')\n",
        "\n",
        "# Convertir delays a float32 para ahorrar memoria\n",
        "float_columns = ['DepDelay', 'ArrDelay']\n",
        "for col in float_columns:\n",
        "    if col in df_optimized.columns:\n",
        "        df_optimized[col] = df_optimized[col].astype('float32')\n",
        "\n",
        "optimization_time = time.time() - start\n",
        "print(f\"Optimizaci√≥n completada en {optimization_time:.3f} segundos\")\n",
        "\n",
        "print(\"\\nTipos de datos optimizados:\")\n",
        "print(df_optimized.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c8482a",
      "metadata": {},
      "source": [
        "## 7. DEMOSTRACI√ìN PR√ÅCTICA: DATASET DE 1GB\n",
        "\n",
        "### Objetivo: Procesar un dataset realista de ~1GB\n",
        "\n",
        "Vamos a:\n",
        "1. **Generar** un dataset de 1GB de datos de e-commerce\n",
        "2. **Analizar** patrones de comportamiento de usuarios\n",
        "3. **Optimizar** queries usando las mejores pr√°cticas de Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca82253",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para generar dataset grande - VERSI√ìN CORREGIDA\n",
        "def generar_dataset_ecommerce(tama√±o_gb=0.5):\n",
        "    \"\"\"\n",
        "    Genera un dataset de e-commerce del tama√±o especificado - VERSI√ìN SIN ERRORES\n",
        "    Estimaci√≥n: ~100MB por mill√≥n de filas\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Generando dataset de {tama√±o_gb}GB...\")\n",
        "    \n",
        "    # Calcular n√∫mero de filas basado en tama√±o deseado\n",
        "    filas_por_mb = 10000  # Estimaci√≥n\n",
        "    total_filas = int(tama√±o_gb * 1000 * filas_por_mb)  # 100MB por mill√≥n de filas\n",
        "    \n",
        "    print(f\"Generando {total_filas:,} filas...\")\n",
        "    \n",
        "    # Crear datos sint√©ticos realistas\n",
        "    np.random.seed(42)  # Para reproducibilidad\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Generar en chunks para memoria eficiente\n",
        "    chunk_size = 100000\n",
        "    chunks = []\n",
        "    \n",
        "    productos = ['Laptop', 'Smartphone', 'Tablet', 'Auriculares', 'Monitor', \n",
        "                'Teclado', 'Mouse', 'Impresora', 'Webcam', 'Micr√≥fono',\n",
        "                'Smartwatch', 'C√°mara', 'Parlantes', 'Disco Duro', 'RAM']\n",
        "    \n",
        "    categorias = ['Electr√≥nicos', 'Accesorios', 'Computadoras', 'Audio', 'Video']\n",
        "    \n",
        "    usuarios = [f'usuario_{i}' for i in range(1, 100001)]  # 100k usuarios √∫nicos\n",
        "    \n",
        "    for i in range(0, total_filas, chunk_size):\n",
        "        current_size = min(chunk_size, total_filas - i)\n",
        "        \n",
        "        # ‚úÖ CORRECCI√ìN: Generar fechas directamente sin shift()\n",
        "        # En lugar de usar shift() que causaba problemas con arrays\n",
        "        fecha_base = pd.Timestamp('2023-01-01')\n",
        "        # Generar offsets aleatorios en minutos\n",
        "        offsets_minutos = np.random.randint(-365*24*60, 0, current_size)\n",
        "        fechas_compra = pd.to_datetime(fecha_base + pd.to_timedelta(offsets_minutos, unit='m'))\n",
        "        \n",
        "        chunk = pd.DataFrame({\n",
        "            'user_id': np.random.choice(usuarios, current_size),\n",
        "            'producto': np.random.choice(productos, current_size),\n",
        "            'categoria': np.random.choice(categorias, current_size),\n",
        "            'precio': np.random.uniform(20, 2000, current_size).round(2),\n",
        "            'cantidad': np.random.poisson(2, current_size) + 1,  # Poisson para naturalidad\n",
        "            'descuento': np.random.uniform(0, 0.3, current_size).round(2),  # 0-30% descuento\n",
        "            'fecha_compra': fechas_compra,  # ‚úÖ SOLUCI√ìN: Fechas generadas directamente\n",
        "            'pais': np.random.choice(['Espa√±a', 'Francia', 'Alemania', 'Italia', 'Portugal'], current_size),\n",
        "            'metodo_pago': np.random.choice(['Tarjeta', 'PayPal', 'Transferencia', 'Bizum'], current_size),\n",
        "            'rating': np.random.randint(1, 6, current_size)  # 1-5 estrellas\n",
        "        })\n",
        "        \n",
        "        # Calcular precio final con descuento\n",
        "        chunk['precio_final'] = (chunk['precio'] * (1 - chunk['descuento']) * chunk['cantidad']).round(2)\n",
        "        \n",
        "        chunks.append(chunk)\n",
        "        \n",
        "        if i % 1000000 == 0 and i > 0:\n",
        "            print(f\"  Generadas {i:,} filas...\")\n",
        "    \n",
        "    # Combinar chunks\n",
        "    df_completo = pd.concat(chunks, ignore_index=True)\n",
        "    \n",
        "    # Calcular tama√±o real\n",
        "    tama√±o_mb = df_completo.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "    \n",
        "    print(f\"‚úÖ Dataset generado en {time.time() - start_time:.2f} segundos\")\n",
        "    print(f\"üìä Filas: {len(df_completo):,}\")\n",
        "    print(f\"üíæ Tama√±o en memoria: {tama√±o_mb:.1f} MB\")\n",
        "    \n",
        "    return df_completo\n",
        "\n",
        "print(\"‚úÖ Funci√≥n corregida lista - Ya no hay errores de array ambiguity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1169dcbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar dataset (AHORA SIN ERRORES)\n",
        "print(\"üî• GENERANDO DATASET...\")\n",
        "df_ecommerce = generar_dataset_ecommerce(tama√±o_gb=0.5)  # 500MB por tiempo del curso\n",
        "print(f\"\\nDataset listo para an√°lisis: {df_ecommerce.shape}\")\n",
        "print(f\"üìã Columnas: {list(df_ecommerce.columns)}\")\n",
        "print(f\"\\nüîç Primeras 3 filas:\")\n",
        "df_ecommerce.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d3640c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir a DataFrame de Dask\n",
        "print(\"üîÑ Convirtiendo a DataFrame de Dask...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Dividir en particiones basadas en el n√∫mero de CPUs\n",
        "import os\n",
        "n_cores = os.cpu_count() or 4\n",
        "n_partitions = min(n_cores * 2, 8)  # Doble n√∫mero de cores, m√°ximo 8\n",
        "\n",
        "print(f\"Usando {n_partitions} particiones en {n_cores} cores\")\n",
        "\n",
        "df_dask_ecommerce = dd.from_pandas(df_ecommerce, npartitions=n_partitions)\n",
        "\n",
        "print(f\"‚úÖ Conversi√≥n completada en {time.time() - start_time:.2f} segundos\")\n",
        "print(f\"Forma: {df_dask_ecommerce.shape}\")\n",
        "print(f\"Particiones: {df_dask_ecommerce.npartitions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a342cf0",
      "metadata": {},
      "source": [
        "### An√°lisis 1: Performance Comparison\n",
        "\n",
        "**Compararemos la velocidad entre Pandas y Dask:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8219f6c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparaci√≥n de performance: Pandas vs Dask\n",
        "print(\"‚ö° COMPARACI√ìN DE PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Operaci√≥n 1: Filtrado simple\n",
        "print(\"\\n1. Filtrado de compras > $100:\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_result = df_ecommerce[df_ecommerce['precio_final'] > 100]\n",
        "pandas_time = time.time() - start\n",
        "print(f\"   Pandas: {pandas_time:.2f} segundos - {len(pandas_result):,} resultados\")\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_result = df_dask_ecommerce[df_dask_ecommerce['precio_final'] > 100].compute()\n",
        "dask_time = time.time() - start\n",
        "print(f\"   Dask:   {dask_time:.2f} segundos - {len(dask_result):,} resultados\")\n",
        "\n",
        "print(f\"   üöÄ Dask es {pandas_time/dask_time:.1f}x m√°s r√°pido\" if dask_time < pandas_time else f\"   üêå Pandas es {dask_time/pandas_time:.1f}x m√°s r√°pido\")\n",
        "\n",
        "# Operaci√≥n 2: GroupBy complejo\n",
        "print(\"\\n2. Ventas por categor√≠a y pa√≠s:\")\n",
        "\n",
        "# Pandas\n",
        "start = time.time()\n",
        "pandas_groupby = df_ecommerce.groupby(['categoria', 'pais'])['precio_final'].agg(['sum', 'mean', 'count'])\n",
        "pandas_time = time.time() - start\n",
        "print(f\"   Pandas: {pandas_time:.2f} segundos\")\n",
        "\n",
        "# Dask\n",
        "start = time.time()\n",
        "dask_groupby = df_dask_ecommerce.groupby(['categoria', 'pais'])['precio_final'].agg(['sum', 'mean', 'count']).compute()\n",
        "dask_time = time.time() - start\n",
        "print(f\"   Dask:   {dask_time:.2f} segundos\")\n",
        "\n",
        "print(f\"   üöÄ Dask es {pandas_time/dask_time:.1f}x m√°s r√°pido\" if dask_time < pandas_time else f\"   üêå Pandas es {dask_time/pandas_time:.1f}x m√°s r√°pido\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1503646b",
      "metadata": {},
      "source": [
        "### An√°lisis 2: Insights de Negocio\n",
        "\n",
        "**Vamos a extraer insights √∫tiles del dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8d3492",
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de negocio con Dask\n",
        "print(\"üìà AN√ÅLISIS DE NEGOCIO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Top productos por ingresos\n",
        "print(\"\\nüèÜ Top 5 productos por ingresos totales:\")\n",
        "start = time.time()\n",
        "top_productos = df_dask_ecommerce.groupby('producto')['precio_final'].sum().nlargest(5).compute()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "for i, (producto, ingresos) in enumerate(top_productos.items(), 1):\n",
        "    print(f\"   {i}. {producto}: ${ingresos:,.2f}\")\n",
        "\n",
        "# 2. An√°lisis temporal\n",
        "print(\"\\nüìÖ Ingresos por mes:\")\n",
        "start = time.time()\n",
        "df_dask_ecommerce['mes'] = df_dask_ecommerce['fecha_compra'].dt.to_period('M')\n",
        "ingresos_mes = df_dask_ecommerce.groupby('mes')['precio_final'].sum().compute()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "print(\"Primeros 5 meses:\")\n",
        "print(ingresos_mes.head())\n",
        "\n",
        "# 3. Segmentaci√≥n de clientes\n",
        "print(\"\\nüë• Segmentaci√≥n por gasto total:\")\n",
        "start = time.time()\n",
        "gasto_usuarios = df_dask_ecommerce.groupby('user_id')['precio_final'].sum().compute()\n",
        "gasto_usuarios = gasto_usuarios.reset_index()\n",
        "gasto_usuarios['segmento'] = pd.cut(gasto_usuarios['precio_final'], \n",
        "                                   bins=[0, 100, 500, 1000, float('inf')], \n",
        "                                   labels=['Bajo', 'Medio', 'Alto', 'VIP'])\n",
        "segmentos = gasto_usuarios['segmento'].value_counts()\n",
        "print(f\"Tiempo: {time.time() - start:.2f} segundos\")\n",
        "print(\"Distribuci√≥n de segmentos:\")\n",
        "for segmento, cantidad in segmentos.items():\n",
        "    porcentaje = (cantidad / len(gasto_usuarios)) * 100\n",
        "    print(f\"   {segmento}: {cantidad:,} usuarios ({porcentaje:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f497f0",
      "metadata": {},
      "source": [
        "## 8. EJERCICIO FINAL: TU TURNO\n",
        "\n",
        "### **Tiempo: 10 minutos**\n",
        "\n",
        "**Objetivo**: Analizar patrones de comportamiento de usuarios usando Dask\n",
        "\n",
        "**Dataset**: Usa el mismo `df_dask_ecommerce` que hemos estado trabajando\n",
        "\n",
        "**Tareas**:\n",
        "1. **Identifica** el top 3 de usuarios m√°s rentables\n",
        "2. **Calcula** el valor promedio de transacci√≥n por m√©todo de pago\n",
        "3. **Encuentra** la correlaci√≥n entre rating y precio final\n",
        "4. **Determina** qu√© d√≠a de la semana genera m√°s ventas\n",
        "\n",
        "**Instrucciones**:\n",
        "- Escribe el c√≥digo completo en la celda siguiente\n",
        "- Usa `.compute()` solo cuando sea necesario\n",
        "- Muestra los resultados de forma clara\n",
        "- Explica qu√© insights obtienes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9692954e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ EJERCICIO FINAL: An√°lisis de Comportamiento de Usuarios\n",
            "============================================================\n",
            "\n",
            "1. üèÜ TOP 3 USUARIOS M√ÅS RENTABLES:\n",
            "\n",
            "2. üí≥ VALOR PROMEDIO POR M√âTODO DE PAGO:\n",
            "\n",
            "3. ‚≠ê CORRELACI√ìN RATING-PRECIO:\n",
            "\n",
            "4. üìÖ MEJOR D√çA DE LA SEMANA:\n",
            "\n",
            "5. üí° TUS INSIGHTS:\n",
            "\n",
            "‚úÖ ¬°Ejercicio completado!\n"
          ]
        }
      ],
      "source": [
        "# üìù EJERCICIO FINAL - Completa el c√≥digo\n",
        "\n",
        "print(\"üéØ EJERCICIO FINAL: An√°lisis de Comportamiento de Usuarios\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tu c√≥digo aqu√≠:\n",
        "\n",
        "# 1. TOP 3 USUARIOS M√ÅS RENTABLES\n",
        "print(\"\\n1. üèÜ TOP 3 USUARIOS M√ÅS RENTABLES:\")\n",
        "# Hint: usa groupby('user_id')['precio_final'].sum() y .nlargest(3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. VALOR PROMEDIO POR M√âTODO DE PAGO\n",
        "print(\"\\n2. üí≥ VALOR PROMEDIO POR M√âTODO DE PAGO:\")\n",
        "# Hint: usa groupby('metodo_pago')['precio_final'].mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. CORRELACI√ìN ENTRE RATING Y PRECIO\n",
        "print(\"\\n3. ‚≠ê CORRELACI√ìN RATING-PRECIO:\")\n",
        "# Hint: usa .corr() entre las columnas 'rating' y 'precio_final'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. D√çA DE LA SEMANA CON M√ÅS VENTAS\n",
        "print(\"\\n4. üìÖ MEJOR D√çA DE LA SEMANA:\")\n",
        "# Hint: extrae day_name() de fecha_compra y agrupa por ese campo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# üí° INSIGHTS ADICIONALES\n",
        "print(\"\\n5. üí° TUS INSIGHTS:\")\n",
        "# ¬øQu√© patrones interesante encuentras?\n",
        "# ¬øQu√© recomendaciones har√≠as al negocio?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ ¬°Ejercicio completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. RESUMEN Y CASOS DE USO\n",
        "\n",
        "### Lo que has aprendido en este curso:\n",
        "\n",
        "1. **Lazy Evaluation**: Dask construye planes de ejecuci√≥n, no ejecuta inmediatamente\n",
        "2. **Particiones**: C√≥mo Dask divide los datos para paralelizaci√≥n eficiente\n",
        "3. **API Familiar**: Sintaxis similar a Pandas pero distribuida y escalable\n",
        "4. **Performance**: Cu√°ndo usar Dask vs Pandas seg√∫n el tama√±o del dataset\n",
        "5. **Optimizaciones**: Tipos de datos, particionamiento y limpieza de datos\n",
        "6. **Casos Pr√°cticos**: An√°lisis real de vuelos con datasets grandes\n",
        "\n",
        "### Casos de uso ideales para Dask:\n",
        "\n",
        "- Datasets de 1GB+ en laptop personal\n",
        "- An√°lisis de logs y datos de sensores\n",
        "- Procesamiento ETL distribuido\n",
        "- Machine Learning con datasets grandes\n",
        "- An√°lisis financiero en tiempo real\n",
        "- Procesamiento de datos cient√≠ficos\n",
        "\n",
        "### Cu√°ndo usar Dask vs Pandas:\n",
        "\n",
        "| Situaci√≥n | Recomendaci√≥n |\n",
        "|-----------|---------------|\n",
        "| Dataset < 100MB | Pandas (m√°s r√°pido) |\n",
        "| Dataset 100MB - 1GB | Dask (si hay m√∫ltiples n√∫cleos) |\n",
        "| Dataset > 1GB | Dask (necesario) |\n",
        "| Datasets que no caben en RAM | Dask (lazy loading) |\n",
        "| An√°lisis simple de una vez | Pandas |\n",
        "| Pipeline complejo distribuido | Dask |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RECURSOS ADICIONALES\n",
        "\n",
        "### Documentaci√≥n y Aprendizaje\n",
        "\n",
        "- Documentaci√≥n oficial: https://docs.dask.org/en/stable/\n",
        "- Tutorial interactivo: https://tutorial.dask.org\n",
        "- Comunidad: https://stackoverflow.com/questions/tagged/dask\n",
        "- Ejemplos reales: https://github.com/dask/dask-examples\n",
        "- Gu√≠a de migraci√≥n de Pandas: https://docs.dask.org/en/stable/dataframe-api.html\n",
        "\n",
        "### Pr√≥ximos Pasos\n",
        "\n",
        "1. **Practica** con tus propios datasets\n",
        "2. **Experimenta** con diferentes n√∫meros de particiones\n",
        "3. **Aprende** Dask-ML para machine learning\n",
        "4. **Explora** Dask-Delayed para funciones personalizadas\n",
        "5. **Considera** clusters distribuidos para datasets masivos"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
